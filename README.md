The workflow could be summarized as several major parts:
1. preprecessing datasets (train, test) and featurization
2. training model with the preprocessed datasets and evluate the models
3. preprocessing commercially availabe database and screening sets, using the trained models as query for virtual screening
4. evaluating the screening result and getting prepared for hit selection

There are also codes aiming for chemical space comparison:
1. PCA analysis
2. max tanimoto score of nearest neighbours
3. Frequency of Murcko Scaffolds in datasets
